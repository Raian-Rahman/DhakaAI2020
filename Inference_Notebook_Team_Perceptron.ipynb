{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference_Notebook_Team_Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raian-Rahman/DhakaAI2020/blob/main/Inference_Notebook_Team_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxKk4sNuScKd"
      },
      "source": [
        "This notebook contains only the inference from our trained weight by Team_Perceptron. Here are some insights that we got from our training. \n",
        "\n",
        "- There were some inconsistency about the training dataset and on second round we corrected many of the labels. We also added some of our captured images in training. \n",
        "- In training, we used yolov5 implementaion by **ultralytics**. As the github repository changes by a lot margin, we downloaded a zip version of the implementation and added it from our drive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcYktlL6N0bK"
      },
      "source": [
        "#  **Uploading the hidden Test File**\n",
        "\n",
        "The hidden test (complete test data) should be uploaded into the following directory.\n",
        "`\n",
        "/content/yolov5/data/hidden_test/\n",
        "`\n",
        "The inference code will test the image from hidden_test folder. \n",
        "\n",
        "**Only the images should be in the hidden_test folder. The organizer can either include it from their drive or manually upload it**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saMEozkBjPoL"
      },
      "source": [
        "#1. Setup \n",
        "In this step the system is setted up for inference. As we stated already, the source code will be downloaded from our drive. The following block does the job. \n",
        "\n",
        "**Here the model name is folded model (we saved it as our implementation)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgpwmMm_bM-p"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!rm -r sample_data\n",
        "!rm -r /content/content/\n",
        "!rm -r /content//yolov5\n",
        "\n",
        "!gdown --id '1PT9iaMtxHTOcI1LGvNdfda-rVSLk5CNj'\n",
        "!unzip FoldedModel.zip; rm FoldedModel.zip;\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN792mwy1fZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a565c1fe-7985-4478-c41f-e4423da05d97"
      },
      "source": [
        "%cd /content/yolov5\n",
        "!pip install -qr requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "\u001b[K     |████████████████████████████████| 645kB 17.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08fseXGqPXqI"
      },
      "source": [
        "The following block creates a folder in data directory for hidden_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqo7HNhuPXca",
        "outputId": "2f9b555b-d1ab-44d2-ffb4-d7d9703d451c"
      },
      "source": [
        "%cd /content/yolov5/data/\n",
        "!mkdir hidden_test"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFPmTZOwC7dW"
      },
      "source": [
        "# 2. Downloading Your Processed Data\n",
        "Here we have included a dataset from which we have trained our model. As we used multiple folding technique, the name of our dataset is fold0,fold1,...,foldN. The main difference between these dataset is the train and validation set. \n",
        "\n",
        "Initially we preprocessed the dataset annotation into **.txt format** and then we folded the dataset by splitting it. \n",
        "\n",
        "The following block downloads the preprocessed dataset. \n",
        "\n",
        "\n",
        "(Metadata is not needed for train in yolov5 but we included it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wD05A7wbRTH"
      },
      "source": [
        "%cd /content/yolov5/data/\n",
        "\n",
        "# !gdown --id 'ID of your preprocessed .zip file'\n",
        "!gdown --id '1U64EuZ1ZghBpue63BN0RoBKnmz4ztTe4'\n",
        "!unzip Fold0.zip; rm Fold0.zip;\n",
        "clear_output()\n",
        "\n",
        "# Setting the metadata files in their perfect place\n",
        "%cp '/content/yolov5/data/metadata/train.txt' /content/yolov5/data/\n",
        "%cp '/content/yolov5/data/metadata/valid.txt' /content/yolov5/data/\n",
        "%cp '/content/yolov5/data/metadata/test.txt' /content/yolov5/data/\n",
        "%cp '/content/yolov5/data/metadata/traffic.names' /content/yolov5/data/\n",
        "%cp '/content/yolov5/data/metadata/traffic.data' /content/yolov5/data/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9zvxw9FCm2r"
      },
      "source": [
        "# 3. Uploading the hidden Test File\n",
        "\n",
        "The hidden test (complete test data) should be uploaded into the following directory.\n",
        "`\n",
        "/content/yolov5/data/hidden_test/\n",
        "`\n",
        "The inference code will test the image from hidden_test folder. \n",
        "\n",
        "**Only the images should be in the hidden_test folder. The organizer can either include it from their drive or manually upload it**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RqUdAgbKal-"
      },
      "source": [
        "# 4. Donwloading Trained Weights: \n",
        "\n",
        "After trained weights are downloaded from our drive to the following directory. \n",
        "`\n",
        "/content/yolov5/weights/\n",
        "`\n",
        "The following block downloads the weights from our drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcUccUb3sOdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4d4ba1-012e-404c-9007-d7c41c4b21da"
      },
      "source": [
        "%cd /content/yolov5/weights/\n",
        "\n",
        "!gdown --id '1--wPks51iiimZdwmf2a0f1e-FvAxDUHm' #res 640\n",
        "!gdown --id '1-ZH_kdA_nZYmI9XiVqCTdsFkXVvkBSIq' #res 1024 fold 1\n",
        "!gdown --id '1-40w_EJTondyT9aD0PPJAmgP1A4ROkEe' #res 1024 fold 2\n",
        "!gdown --id '187eJgVdAKcMeO-G5KkLOtynryV65h-Wq' #res 1024 complete corrected dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5/weights\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--wPks51iiimZdwmf2a0f1e-FvAxDUHm\n",
            "To: /content/yolov5/weights/w4.pt\n",
            "178MB [00:02, 78.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-ZH_kdA_nZYmI9XiVqCTdsFkXVvkBSIq\n",
            "To: /content/yolov5/weights/w3.pt\n",
            "710MB [00:15, 45.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-40w_EJTondyT9aD0PPJAmgP1A4ROkEe\n",
            "To: /content/yolov5/weights/w2.pt\n",
            "710MB [00:17, 40.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=187eJgVdAKcMeO-G5KkLOtynryV65h-Wq\n",
            "To: /content/yolov5/weights/w1.pt\n",
            "710MB [02:30, 2.19MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "komO8CbeyJwy"
      },
      "source": [
        "# 5. Submission\n",
        "\n",
        "This part of the code deals with making the submission file. Remember, the actual yolo v5 model outpus in the format  `(object-id) (score) (x-centre) (y-centre) (width) (height)` in each line for each object in the text label file. However, in the competition, you need to submit in this format `(image_name) (object_name) (xmin) (ymin) (xmax) (ymax) (width) (height)` for each line for each object in a `.csv` file.\n",
        "\n",
        "So, first the inference is run and then the CSV is generated. **CSV is stored inside the yolov5 folded along with the timestamp**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAHNsHXKvbSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66e1314-0aa9-492b-e937-837130be1bbc"
      },
      "source": [
        "%cd /content/yolov5/\n",
        "from models import *  \n",
        "from utils.datasets import *\n",
        "from utils.utils import *\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import (\n",
        "    check_img_size, non_max_suppression, apply_classifier, scale_coords,\n",
        "    xyxy2xywh, strip_optimizer, set_logging)\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
        "\n",
        "def detect(save_img=False):\n",
        "    imgsz = opt.img_size \n",
        "    out, source, weights, half, view_img, save_txt = opt.output, opt.source, opt.weights, opt.half, opt.view_img, opt.save_txt\n",
        "\n",
        "    # Initialize\n",
        "    device = torch_utils.select_device(opt.device)\n",
        "    if os.path.exists(out):\n",
        "        shutil.rmtree(out)  # delete output folder\n",
        "    os.makedirs(out)  # make new output folder\n",
        "\n",
        "    # Initialize model\n",
        "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "    imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
        "    if half:\n",
        "        model.half()  # to FP16\n",
        "\n",
        "    \n",
        "    \n",
        "    # Eval mode\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Half precision\n",
        "    half = half and device.type != 'cpu'  # half precision only supported on CUDA\n",
        "    if half:\n",
        "        model.half()\n",
        "\n",
        "    # Set Dataloader\n",
        "    vid_path, vid_writer = None, None\n",
        "    save_img = True\n",
        "    dataset = LoadImages(source, img_size=imgsz)\n",
        "\n",
        "    # Get names and colors\n",
        "    names = load_classes(opt.names)\n",
        "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
        "\n",
        "    # Run inference\n",
        "    t0 = time.time()\n",
        "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
        "    _ = model(img.half() if half else img.float()) if device.type != 'cpu' else None  # run once\n",
        "\n",
        "    results=[]\n",
        "    for path, img, im0s, vid_cap in dataset:\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "            img = img.unsqueeze(0)\n",
        "\n",
        "        # Inference\n",
        "        t1 = torch_utils.time_synchronized()\n",
        "        pred = model(img, augment=opt.augment)[0]\n",
        "        t2 = torch_utils.time_synchronized()\n",
        "\n",
        "        # to float\n",
        "        if half:\n",
        "            pred = pred.float()\n",
        "\n",
        "        # Apply NMS\n",
        "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
        "        \n",
        "        # Process detections\n",
        "        for i, det in enumerate(pred):  # detections for image i\n",
        "            \n",
        "            p, s, im0 = path, '', im0s          \n",
        "\n",
        "            save_path = str(Path(out) / Path(p).name)\n",
        "            #print(p)\n",
        "            s += '%gx%g ' % img.shape[2:]  # print string\n",
        "            #print(s)\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh\n",
        "            if det is not None and len(det):\n",
        "                # Rescale boxes from imgsz to im0 size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
        "\n",
        "                xmin = []\n",
        "                ymin = []\n",
        "                xmax = []\n",
        "                ymax = []\n",
        "                scores = []\n",
        "                labels_value=[]\n",
        "                image_ids=[]\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    if save_txt:  # Write to file\n",
        "                        \n",
        "                        conf_score = '%.2f' % (conf)\n",
        "                        label_with_cls = '%s' % (names[int(cls)])\n",
        "                        \n",
        "                        labels_value.append(label_with_cls)\n",
        "                        \n",
        "                        xmin.append(int(xyxy[0]))\n",
        "                        ymin.append(int(xyxy[1]))\n",
        "                        xmax.append(int(xyxy[2]))\n",
        "                        ymax.append(int(xyxy[3]))\n",
        "                        \n",
        "                        scores.append(conf_score)\n",
        "                        image_ids.append(save_path)\n",
        "\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        with open(save_path[:save_path.rfind('.')] + '.txt', 'a') as file:\n",
        "                            file.write(('%g ' * 5 + '\\n') % (cls, *xywh))  # label format\n",
        "\n",
        "                    if save_img or view_img:  # Add bbox to image\n",
        "                        label = '%s %.2f' % (names[int(cls)], conf)\n",
        "                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)])\n",
        "\n",
        "            # Print time (inference + NMS)\n",
        "            print('%sDone. (%.3fs)' % (s, t2 - t1))\n",
        "\n",
        "            # Stream results\n",
        "            if view_img:\n",
        "                cv2.imshow(p, im0)\n",
        "                if cv2.waitKey(1) == ord('q'):  # q to quit\n",
        "                    raise StopIteration\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'images':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "        result = {\n",
        "            'image_id': image_ids,\n",
        "            'score': scores,\n",
        "            'class': labels_value,\n",
        "            'xmin': xmin,\n",
        "            'ymin': ymin,\n",
        "            'xmax': xmax,\n",
        "            'ymax': ymax\n",
        "\n",
        "            }\n",
        "\n",
        "        results.append(result)\n",
        "    if save_txt or save_img:\n",
        "        print('Results saved to %s' % os.getcwd() + os.sep + out)\n",
        "\n",
        "    print('Done. (%.3fs)' % (time.time() - t0))\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICmBtBuQG1wK"
      },
      "source": [
        "The hyperparameters for inference is setted in this class. **PLEASE DON'T CHANGE ANYTHING ON THE FOLLOWING BLOCK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwFzjW7Nyrsu"
      },
      "source": [
        "# Setting up parameters for inference /submission\n",
        "class opt:\n",
        "    cfg='/content/yolov5/models/yolov5x.yaml'\n",
        "    names='/content/yolov5/data/traffic.names'\n",
        "    weights= ['/content/yolov5/weights/w1.pt', \n",
        "              '/content/yolov5/weights/w2.pt', \n",
        "              '/content/yolov5/weights/w3.pt', \n",
        "              '/content/yolov5/weights/w4.pt']\n",
        "    source='/content/yolov5/data/hidden_test' #directory to the hidden test data\n",
        "    save_txt=True\n",
        "    output='output'  # Output directory of the results\n",
        "    classes=False\n",
        "    img_size=1024    # Inference Imag Size\n",
        "    conf_thres=0.25\n",
        "    iou_thres=0.55\n",
        "    fourcc='mp4v'\n",
        "    half=False\n",
        "    device=''\n",
        "    view_img=False\n",
        "    agnostic_nms=False\n",
        "    augment=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdQNj5GCyuyw"
      },
      "source": [
        "# predict results\n",
        "with torch.no_grad():\n",
        "    res=detect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSaSc4fdak1q"
      },
      "source": [
        "In the submission file you need to have to following headers with values\n",
        "\n",
        "\n",
        "\n",
        "*   `image_id:` image file name\n",
        "*   `class:` object name like `rickshaw` or `bus`\n",
        "*   `score:` confidence score of the object detection\n",
        "*   `xmin:` \n",
        "*   `ymin`\n",
        "*   `xmax`\n",
        "*   `ymax`\n",
        "*   `height`\n",
        "*   `width`\n",
        "\n",
        "**As we resized all the test images into 1024 by 1024, so the output is scaled by 1024 by 1024**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqtkEt8wH-bp"
      },
      "source": [
        "The following block generates the csv file which will be saved in the `/content/yolov5/` directory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbZgmn7Ly6D5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "append_data=[]\n",
        "\n",
        "for i in range(len(res)):\n",
        "  df = pd.DataFrame(res[i], columns = ['image_id','class','score','xmin','ymin','xmax','ymax'])\n",
        "  append_data.append(df)\n",
        "\n",
        "finl_results=pd.concat(append_data)\n",
        "finl_results.image_id = [x.strip('output/') for x in finl_results.image_id]\n",
        "\n",
        "finl_results['width'] = 1024\n",
        "finl_results['height'] = 1024\n",
        "\n",
        "print(f'Number of Objects Detected: {finl_results.shape[0]}')\n",
        "\n",
        "# Saving the submission file with timestamp\n",
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "finl_results.to_csv(f'Team_Perceprtron {timestamp} .csv', index=False)\n",
        "print('Submission file is written Successfully!\\n\\n Sneak Peak at Submission File :)')\n",
        "\n",
        "finl_results.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}